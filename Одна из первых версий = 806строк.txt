#https://repl.it/@Egor9731/Egor9731diplChastotniki


####################################################################################
####################################################################################
####################################################################################
# Версия 15.ноя.17 17.55

#import random # пока не нужен
import timeit # для таймера
import inspect # Для Print_current_func_name()
from collections import OrderedDict # Для сортировки словарей sort_dict() 1
from operator import itemgetter # Для сортировки словарей sort_dict() 2
#from copy import deepcopy # Для копирования словаря при обрезке краев (сделал по-другому)



####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################


# Исхдный текст (затычка)(можно полностью убирать)
#begin_text = "1 2.1 1 2 4.. 3 4   '5 .6 \ 123"
#begin_text = "Версия Террарии 1.3.0.1 (часто называемая Версией 1.3) — это крупное обновление, с которым в игру вошло большое число новых предметов и персонажей, новые боссы и события, совершенно новый Экспертный режим, а также большое число значительных изменении механики и интерфейса. Большое количество уже существующих элементов также изменили для соответствия игровому процессу и балансу. Версия 1.3 создана такой, чтобы быть \"лучше\", а не просто больше и с увеличенным количеством предметов. Тем не менее, в обновление входят более 800 новых предметов, бо?льшая часть которых не была включена в список изменений, чтобы игроки нашли их самостоятельно."
#begin_text += " Террарии Террарии Террарии Террарии Террарии Террарии Террарии 123"

begin_text = ""

#https://habrahabr.ru/company/abbyy/blog/211707/       476 слов   5%=23
begin_text += "Еще с 1950-х годов в сфере создания искусственного интеллекта выделилось два подхода — символьные вычисления и коннекционизм. Символьные вычисления – это направление, основанное на моделировании мышления человека, а коннекционизм — на моделировании устройства мозга.Первыми достижениями в области символьных вычислений были созданный в 50-е годы язык Lisp и работа Дж. Робинсона в области логического вывода. В коннекционизме таковым стало создание персептрона – самообучающегося линейного классификатора, моделирующего работу нейрона. Дальнейшие яркие достижения находились в основном в русле символьной парадигмы. В частности, это работы Сеймура Пайперта и Роберта Антона Уинсона в области психологии восприятия и, конечно, фреймы Марвина Минского.В 70-е годы появились первые прикладные системы, использующие элементы искусственного интеллекта – экспертные системы. Дальше произошел некий ренессанс коннекционизма с появлением многослойных нейронных сетей и алгоритма их обучения методом обратного распространения. В 80-е годы увлечение нейронными сетями было просто повальным. Сторонники этого подхода обещали создать нейрокомпьютеры, которые будут работать практически как человеческий мозг.Но ничего особенного из этого не вышло, потому что настоящие нейроны устроены намного сложнее, чем формальные, на которых основаны многослойные нейросети. И количество нейронов в человеческом мозге тоже намного больше, чем можно было позволить себе в нейросети. Основное, для чего оказались пригодны многослойные нейросети – это решение задачи классификации.Следующей популярной парадигмой в области искусственного интеллекта стало машинное обучение. Подход начал бурно развиваться с конца 80-х годов и не теряет популярности и поныне. Значительный толчок развитию машинного обучения дало появление интернета и большого количества разнообразных легкодоступных данных, которые можно использовать для обучения алгоритмов.Главные задачи при проектировании искусственного интеллектаМожно проанализировать, что роднит те задачи, которые относятся к искусственному интеллекту. Несложно заметить, что общее в них — отсутствие известной, четко определенной процедуры решения. Этим, собственно, задачи, относящиеся к AI, отличаются от задач теории компиляции или вычислительной математики. Интеллектуальные системы ищут субоптимальные решения задачи. Нельзя ни доказать, ни гаратировать, что найденное искусственным интеллектом решение будет строго оптимальным. Тем не менее, в большинстве практических задач субоптимальные решения всех устраивают. Более того, нужно помнить, что и человек практически никогда не решает задачу оптимально. Скорее, наоборот. Возникает очень важный вопрос: как может AI решить задачу, для которой нет алгоритма решения? Суть в том, чтобы делать это так же, как и человек — выдвигать и проверять правдоподобные гипотезы. Естественно, что для выдвижения и проверки гипотез нужны знания. imageЗнания — это описание предметной области, в которой работает интеллектуальная система. Если перед нами система распознавания символов естественного языка, то знания включают в себя описания устройства символов, структуру текста и тех или иных свойств языка. Если это система оценки кредитоспособности клиента, у нее должны быть знания о типах клиентов и знания о том, как профиль клиента связан с его потенциальной некредитоспособностью. Знания бывают двух типов – о предметной области и о поиске путей решения (метазнания). Основные задачи проектирования интеллектуальной системы сводятся к выбору способов представления знаний, способов получения знаний и способов применения знаний. Представление знанийСуществуют два основных способа представления знаний — декларативные и процедурные. Декларативные знания могут быть представлены в структурированном или в неструктурированном виде. Структурированные представления – это та или иная разновидность фреймового подхода. Семантические сети или формальные грамматики, которые тоже можно считать разновидностями фреймов. Знания в этих формализмах представлены в виде множества объектов и отношений между ними. Неструктурированные представления используются обычно в тех сферах, которые связаны с решением задач классификации. Это обычно векторы оценок весовых коэффициентов, вероятностей и тому подобное. Практически все способы структурированного представления знания базируются на формализме фреймов, которые в 1970-е ввел Марвин Минский из MIT, чтобы обозначить структуру знаний для восприятия пространственных сцен. Как выяснилось, подобный подход годится практически для любой задачи. Фрейм состоит из имени и отдельных единиц, называемых слотами. Значением слота может быть, в свою очередь, ссылка на другой фрейм… Фрейм может быть потомком другого фрейма, наследуя у него значения слотов. При этом потомок может переопределять значения слотов предка и добавлять новые. Наследование используется для того, чтобы сделать описание более компактным и избежать дублирования. Несложно заметить, что существует сходство между фреймами и объектно-ориентированным программированием, где фрейму соответствует объект, а слоту — поле. Сходство это неслучайное, потому что фреймы были одним из источников возникновения ООП. В частности, один из первых объектно-ориентированных языков Small Talk практически в точности реализовывал фреймовые представления объектов и классов.Для процедурного представления знаний используются продукции или продукционные правила. Продукционная модель — это модель, основанная на правилах, позволяющих представить знание в виде предложений «условие — действие». Такой подход раньше был популярен в различных системах диагностики. Достаточно естественно в виде условия описывать симптомы, проблемы или неисправности, а в виде действия — возможную неисправность, которая приводит к наличию этих симптомов. В следующей статье мы поговорим о способах применения знаний."


#https://habrahabr.ru/company/abbyy/blog/217839/           536 слов   5%=26
begin_text += " Мы продолжаем серию постов про искусственный интеллект, написанных по мотивам выступления в «Технопарке» Mail.ru Константина Анисимовича — директора департамента разработки технологий ABBYY. Вторая статья будет посвящена алгоритмам поиска.Навигатор по серии постов:В зависимости от того, какой способ представления знаний мы выбрали — декларативный или продукционный — мы определяем способ применения знаний. С продукционной системой все достаточно просто: мы непосредственно интерпретируем продукции.Если же мы выбрали декларативное представление знаний, то все происходит несколько сложнее. Для этого нам нужно реализовать поиск в пространстве состояний. Дело в том, что структурированное представлений знаний организовано иерархически. А если мы пытаемся применить иерархическое описание к входным данным, то мы на каждом его уровне получим возможные варианты интерпретации данных — гипотезы. Для того чтобы эффективно выбирать одну или несколько лучших гипотез, избегая комбинаторного взрыва, используются алгоритмы поиска в пространстве состояний. Пространство состояний возникает при разбиении решения задачи на отдельные шаги. Здесь выделяется начальное состояние, когда мы еще не выбрали гипотезу, а также конечное состояние, когда мы нашли гипотезу, которая является допустимым решением нашей задачи. В процессе поиска этой гипотезы есть операции перехода в следующее состояние. При этом не сложно заметить, что пространственное состояние имеет экспоненциальную сложность в зависимости от числовых шагов. Если задаться целью обойти пространство состояний целиком, потребуется экспоненциальное время. Чтобы избежать этой проблемы, применяются различные алгоритмы поиска. Они делятся на два типа – полный перебор и эвристический поиск. Как уже ясно, полный перебор в подавляющем большинстве случаев не годится (если только мы не решаем очень маленькую задачу), но имеет некую учебную ценность. Поэтому мы рассмотрим сначала его, а вы, если хорошо с ним знакомы, можете сразу перейти к эвристическому типу.Поиск с помощью полного перебора Существуют следующие виды полного перебора: поиск в ширину, поиск в глубину и поиск с итеративным углублением. Во время поиска в ширину мы делаем все возможные шаги в пространственном состоянии изначального состояния и получаем новое множество состояний. Дальше из каждого состояния этого множества снова делаем все всевозможные шаги, получаем следующее множество состояний и так далее – пока не найдем решение. Графически поиск в ширину можно представить в виде движения фронта в пространстве состояний. Поиск в глубину означает, что из начального состояния мы делаем один некий шаг, дальше из нового состояния делаем еще один шаг и т.д., пока не дойдем до конечного состояния или до состояния, из которого нельзя больше сделать ни одного шага. В этом случае мы рекурсивно возвращаемся назад и снова делаем шаги из того состояния, в которое вернулись, пока не найдем решение. Несложно заметить, что такой поиск в ширину имеет экспоненциальную сложность как по времени, так и по памяти. А поиск в глубину имеет экспоненциальную сложность по времени. Конечно, нам может повезти, и решение найдется сразу, но на практике это маловероятно. Поиск с итеративным углублением — это оптимизация поиска в глубину и в ширину, которая гарантированно позволяет найти самое близкое к начальному состоянию решение, избегая экспоненциальной сложности. Каким образом реализуется этот алгоритм? Мы ищем в глубину с ограничением глубины константой N. Нашли решение – хорошо. Не нашли — повторяем поиск в глубину с константой N+1 и так далее, пока не отыщется. Этот алгоритм, хотя и несложен в реализации, пригоден только для самых простейших задач. Эвристический поиск Чаще всего практические задачи решаются с помощью эвристического поиска. Эвристический поиск основан на функции оценки состояния. В отличие от алгоритмов полного перебора, эвристический поиск позволяет ранжировать пространственные состояния на основе их «перспективности». Эвристический поиск ищет в пространстве состояний более целенаправленно, чем алгоритмы полного перебора. Важно, что во многих задачах оценка состояния есть наилучшая оценка пути достижения данного состояния из начального. Если в нашей задаче возможна такая оценка, то алгоритмы эвристического поиска значительно упрощаются. Основной класс алгоритмов эвристического поиска – это поиск от наилучшего состояния. Он включает три основных алгоритма: это жадный поиск, лучевой поиск и А*. Общая их идея основана на поддержании в процессе поиска множества достигнутых состояний и выборе на каждом шаге одного или нескольких лучших состояний. Простейший из них — жадный поиск. Если его применить в задаче поиска наилучшего пути в графе, это даст известный алгоритм Дейкстры. Жадный поиск, выбирая состояние, из которого будет продолжаться поиск, ищет состояние с наилучшей оценкой пути от начального в данное. Поэтому он и называется «жадным» — поскольку «хватает» самое лучшее на данный момент состояние, не думая о последствиях. Естественно, как и многие жадные алгоритмы, такая стратегия не приводит к оптимальному решению. Конечные состояния зачастую достигаются слишком длинным, неоптимальным путем. Кроме того, жадный поиск постоянно должен поддерживать множества всех достигнутых состояний, которых может быть слишком много, отчего чрезмерно расходуется память. Поиск пути с помощью алгоритма Дейкстры. Поиск был запущен из красной вершины в синюю, фиолетовые были посещены, серые – нет.Алгоритм лучевого поиска и алгоритм А* — это попытки улучшить поведение жадного поиска и исправить эти две присущие ему проблемы. Лучевой поиск работает следующим образом: на каждом шаге мы поддерживаем некоторое множество из N лучших состояний. Далее из каждого из этих состояний делаем все возможные шаги и получаем множество состояний следующего поколения. В этом множестве мы удаляем дубликаты, то есть одинаковые состояния. Оцениваем оставшиеся и сортируем в порядке ухудшения оценки. Далее выбираем N лучших, и так до тех пор, пока мы не найдем интересующее нас конечное состояние. У лучевого поиска есть свои достоинства и недостатки. Основной его минус в том, что, в отличие от жадного, лучевой поиск не гарантирует нахождения конечного состояния с наилучшим качеством, потому что в процессе движения фронта лучшее состояние может из него выпасть. На практике с этим можно бороться, настраивая ширину фронта. Чем фронт уже, тем быстрее работает алгоритм, но тем чаще он ошибается. Чем шире фронт, тем алгоритм работает лучше, но дольше. Это одно из его важных преимуществ – возможность легко балансировать между скоростью и качеством. Лучевой поиск очень популярен в академической среде, особенно среди китайских ученых. Он прост в реализации и работает достаточно неплохо. Идея алгоритма А* заключается в том, что мы выбираем на каждом шаге не лучшее, а наиболее перспективное на данный момент состояние. То состояние, через которое с наибольшей вероятностью проходит путь до лучшего конечного состояния.К оценке текущего состояния в алгоритме А* добавляется эвристическая оценка снизу остатка пути до конечного состояния. Если эвристическая оценка снизу достаточно хороша, то А* работает эффективно и быстро находит наилучшее состояние. В этом случае А* будет полиноминальным по числу шагов, а не экспоненциальным. А* — это действительно хороший алгоритм, в своей работе я использую его гораздо чаще, чем лучевой поиск. Поиск пути с помощью алгоритма А*Недостатком алгоритма А* является необходимость придумать эвристику. Справедливости ради нужно сказать, что во многих задачах эта эвристика находится достаточно легко и естественно. В таких задачах и рекомендуется использовать А*. Если эвристику придумать не получается, тогда на помощь вам приходит лучевой поиск.В третьем посте данной серии речь пойдет о способах получения знаний, используемых при создании искусственного интеллекта, а также о машинном обучении — в частности, о тех методах, которые используются в FineReader."


#https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C
# ОЧЕНЬ много текста
#begin_text += "Искусственная нейронная сеть[править | править вики-текст]Материал из Википедии — свободной энциклопедии У этого термина существуют и другие значения, см. Нейронная сеть (значения).Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон Иску?сственная нейро?нная се?ть (ИНС) — математическая модель, а также её программное или аппаратное воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма. Это понятие возникло при изучении процессов, протекающих в мозге, и при попытке смоделировать эти процессы. Первой такой попыткой были нейронные сети У. Маккалока и У. Питтса[1]. После разработки алгоритмов обучения получаемые модели стали использовать в практических целях: в задачах прогнозирования, для распознавания образов, в задачах управления и др.ИНС представляют собой систему соединённых и взаимодействующих между собой простых процессоров (искусственных нейронов). Такие процессоры обычно довольно просты (особенно в сравнении с процессорами, используемыми в персональных компьютерах). Каждый процессор подобной сети имеет дело только с сигналами, которые он периодически получает, и сигналами, которые он периодически посылает другим процессорам. И, тем не менее, будучи соединёнными в достаточно большую сеть с управляемым взаимодействием, такие по отдельности простые процессоры вместе способны выполнять довольно сложные задачи.С точки зрения машинного обучения, нейронная сеть представляет собой частный случай методов распознавания образов, дискриминантного анализа, методов кластеризации и т. п.С математической точки зрения, обучение нейронных сетей — это многопараметрическая задача нелинейной оптимизации.С точки зрения кибернетики, нейронная сеть используется в задачах адаптивного управления и как алгоритмы для робототехники.С точки зрения развития вычислительной техники и программирования, нейронная сеть — способ решения проблемы эффективного параллелизма[2].А с точки зрения искусственного интеллекта, ИНС является основой философского течения коннективизма и основным направлением в структурном подходе по изучению возможности построения (моделирования) естественного интеллекта с помощью компьютерных алгоритмов.Нейронные сети не программируются в привычном смысле этого слова, они обучаются. Возможность обучения — одно из главных преимуществ нейронных сетей перед традиционными алгоритмами. Технически обучение заключается в нахождении коэффициентов связей между нейронами. В процессе обучения нейронная сеть способна выявлять сложные зависимости между входными данными и выходными, а также выполнять обобщение. Это значит, что в случае успешного обучения сеть сможет вернуть верный результат на основании данных, которые отсутствовали в обучающей выборке, а также неполных и/или «зашумленных», частично искажённых данных.Содержание  [скрыть]1	Хронология 2	Известные применения 2.1	Распознавание образов и классификация 2.1.1	Используемые архитектуры нейросетей 2.2	Принятие решений и управление2.2.1	Используемые архитектуры нейросетей 2.3	Кластеризация 2.3.1	Используемые архитектуры нейросетей 2.4	Прогнозирование 2.4.1	Используемые архитектуры нейросетей 2.5	Аппроксимация 2.5.1	Используемые архитектуры нейросетей 2.6	Сжатие данных и Ассоциативная память 2.6.1	Используемые архитектуры нейросетей 2.7	Анализ данных 2.7.1	Используемые архитектуры нейросетей 2.8	Оптимизация 2.8.1	Используемые архитектуры нейросетей 3	Этапы решения задач 3.1	Сбор данных для обучения 3.2	Выбор топологии сети 3.3	Экспериментальный подбор характеристик сети 3.4	Экспериментальный подбор параметров обучения 3.5	Обучение сети 6	Проверка адекватности обучения 4	Классификация по типу входной информации 5	Классификация по характеру обучения 6	Классификация по характеру настройки синапсов 7	Классификация по времени передачи сигнала 8	Классификация по характеру связей 8.1	Сети прямого распространения (Feedforward) 8.2	Рекуррентные нейронные сети 8.3	Радиально-базисные функции 8.4	Самоорганизующиеся карты 9	Известные типы сетей 10	Отличия от машин с архитектурой фон Неймана 11	Примеры приложений 11.1	Предсказание финансовых временных рядов 11.2	Психодиагностика 11.3	Хемоинформатика 11.4	Нейроуправление 11.5	Экономика 12	См. также 13	Примечания 14	Литература 15	СсылкиХронология[править | править вики-текст]1943 — У. Маккалок и У. Питтс формализуют понятие нейронной сети в фундаментальной статье о логическом исчислении идей и нервной активности[1]. В начале своего сотрудничества с Питтсом Н. Винер предлагает ему вакуумные лампы в качестве идеального на тот момент средства для реализации эквивалентов нейронных сетей[3].1948 — Н. Винер вместе с соратниками публикует работу о кибернетике. Основной идеей является представление сложных биологических процессов математическими моделями.1949 — Д. Хебб предлагает первый алгоритм обучения.В 1958 Ф. Розенблатт изобретает однослойный перцептрон и демонстрирует его способность решать задачиклассификации[4]. Перцептрон обрёл популярность — его используют для распознавания образов, прогнозирования погоды и т. д.; в то время казалось, что уже не за горами создание полноценного искусственного интеллекта. К моменту изобретения перцептрона завершилось расхождение теоретических работ Маккалока с т. н. «кибернетикой» Винера; Маккалок и его последователи вышли из состава «Кибернетического клуба».В 1960 году Уидроу (англ.) совместно со своим студентом Хоффом на основе дельта-правила (формулы Уидроу) разработали Адалин, который сразу начал использоваться для задач предсказания и адаптивного управления. Адалин был построен на базе созданных ими же (Уидроу — Хоффом) принципиально новых элементах — мемисторах[5]. Сейчас Адалин (адаптивный сумматор) является стандартным элементом многих систем обработки сигналов[6].В 1963 году в Институте проблем передачи информации АН СССР. А. П. Петровым проводится подробное исследование задач «трудных» для перцептрона[7]. Эта пионерская работа в области моделирования ИНС в СССР послужила отправной точкой для комплекса идей М. М. Бонгарда — как «сравнительно небольшой переделкой алгоритма (перцептрона) исправить его недостатки»[8]. Работы А. П. Петрова и М. М. Бонгарда весьма способствовали тому, что в СССР первая волна эйфории по поводу ИНС была сглажена.В 1969 году М. Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он неспособен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений. Интерес к нейронным сетям резко спадает.В 1972 году Т. Кохонен и Дж. Андерсон[en] независимо предлагают новый тип нейронных сетей, способных функционировать в качестве памяти[9].В 1973 году Б. В. Хакимов предлагает нелинейную модель с синапсами на основе сплайнов и внедряет её для решения задач в медицине, геологии, экологии[10].1974 — Пол Дж. Вербос[11] и А. И. Галушкин[12] одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов[уточнить]. Изобретение не привлекло особого внимания.1975 — Фукусима представляет когнитрон — самоорганизующуюся сеть, предназначенную для инвариантного распознавания образов, но это достигается только при помощи запоминания практически всех состояний образа.1982 — после периода забвения, интерес к нейросетям вновь возрастает. Дж. Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (так называемая сеть Хопфилда). Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.1986 — Дэвидом И. Румельхартом, Дж. Е. Хинтоном и Рональдом Дж. Вильямсом[13] и независимо и одновременно С. И. Барцевым и В. А. Охониным (Красноярская группа)[14] переоткрыт и существенно развит метод обратного распространения ошибки. Начался взрыв интереса к обучаемым нейронным сетям.2007 Джеффри Хинтоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Успех обусловлен тем, что Хинтон при обучении нижних слоев сети использовал ограниченную машину Больцмана (RBM — Restricted Boltzmann Machine). Глубокое обучение по Хинтону — это очень медленный процесс. Необходимо использовать много примеров распознаваемых образов (например, множество лиц людей на разных фонах). После обучения получается готовое быстро работающее приложение, способное решать конкретную задачу (например, осуществлять поиск лиц на изображении). Функция поиска лиц людей на сегодняшний день стала стандартной и встроена во все современные цифровые фотоаппараты. Технология глубокого обучения активно используется интернет-поисковиками при классификации картинок по содержащимся в них образам. Применяемые при распознавании искусственные нейронные сети могут иметь до 9 слоев нейронов, их обучение ведётся на миллионах изображений с отыскиваемым образом.Известные применения[править | править вики-текст]Распознавание образов и классификация[править | править вики-текст]Основные статьи: Теория распознавания образов, Задача классификацииВ качестве образов могут выступать различные по своей природе объекты: символы текста, изображения, образцы звуков и т. д. При обучении сети предлагаются различные образцы образов с указанием того, к какому классу они относятся. Образец, как правило, представляется как вектор значений признаков. При этом совокупность всех признаков должна однозначно определять класс, к которому относится образец. В случае, если признаков недостаточно, сеть может соотнести один и тот же образец с несколькими классами, что неверно. По окончании обучения сети ей можно предъявлять неизвестные ранее образы и получать ответ о принадлежности к определённому классу.Топология такой сети характеризуется тем, что количество нейронов в выходном слое, как правило, равно количеству определяемых классов. При этом устанавливается соответствие между выходом нейронной сети и классом, который он представляет. Когда сети предъявляется некий образ, на одном из её выходов должен появиться признак того, что образ принадлежит этому классу. В то же время на других выходах должен быть признак того, что образ данному классу не принадлежит[15]. Если на двух или более выходах есть признак принадлежности к классу, считается, что сеть «не уверена» в своём ответе.Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронОбучение без учителя:Сети адаптивного резонансаСмешанное обучение:Сеть радиально-базисных функцийПринятие решений и управление[править | править вики-текст]Эта задача близка к задаче классификации. Классификации подлежат ситуации, характеристики которых поступают на вход нейронной сети. На выходе сети при этом должен появиться признак решения, которое она приняла. При этом в качестве входных сигналов используются различные критерии описания состояния управляемой системы[16].Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронСмешанное обучение:Сеть радиально-базисных функцийКластеризация[править | править вики-текст]Основная статья: Кластерный анализПод кластеризацией понимается разбиение множества входных сигналов на классы, при том, что ни количество, ни признаки классов заранее не известны. После обучения такая сеть способна определять, к какому классу относится входной сигнал. Сеть также может сигнализировать о том, что входной сигнал не относится ни к одному из выделенных классов — это является признаком новых, отсутствующих в обучающей выборке, данных. Таким образом, подобная сеть может выявлять новые, неизвестные ранее классы сигналов. Соответствие между классами, выделенными сетью, и классами, существующими в предметной области, устанавливается человеком. Кластеризацию осуществляют, например, нейронные сети Кохонена.Нейронные сети в простом варианте Кохонена не могут быть огромными, поэтому их делят на гиперслои (гиперколонки) и ядра (микроколонки). Если сравнивать с мозгом человека, то идеальное количество параллельных слоёв не должно быть более 112. Эти слои в свою очередь составляют гиперслои (гиперколонку), в которой от 500 до 2000 микроколонок (ядер). При этом каждый слой делится на множество гиперколонок, пронизывающих насквозь эти слои. Микроколонки кодируются цифрами и единицами с получением результата на выходе. Если требуется, то лишние слои и нейроны удаляются или добавляются. Идеально для подбора числа нейронов и слоёв использовать суперкомпьютер. Такая система позволяет нейронным сетям быть пластичными.Используемые архитектуры нейросетей[править | править вики-текст]Обучение без учителя:ПерцептронСамоорганизующаяся карта КохоненаНейронная сеть КохоненаСети адаптивного резонансаПрогнозирование[править | править вики-текст]Основная статья: ПрогнозированиеСпособности нейронной сети к прогнозированию напрямую следуют из её способности к обобщению и выделению скрытых зависимостей между входными и выходными данными. После обучения сеть способна предсказать будущее значение некой последовательности на основе нескольких предыдущих значений и (или) каких-то существующих в настоящий момент факторов. Следует отметить, что прогнозирование возможно только тогда, когда предыдущие изменения действительно в какой-то степени предопределяют будущие. Например, прогнозирование котировок акций на основе котировок за прошлую неделю может оказаться успешным (а может и не оказаться), тогда как прогнозирование результатов завтрашней лотереи на основе данных за последние 50 лет почти наверняка не даст никаких результатов.Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронСмешанное обучение:Сеть радиально-базисных функцийАппроксимация[править | править вики-текст]Основная статья: АппроксимацияНейронные сети могут аппроксимировать непрерывные функции. Доказана обобщённая аппроксимационная теорема[17]: с помощью линейных операций и каскадного соединения можно из произвольного нелинейного элемента получить устройство, вычисляющее любую непрерывную функцию с некоторой наперёд заданной точностью. Это означает, что нелинейная характеристика нейрона может быть произвольной: от сигмоидальной до произвольного волнового пакета или вейвлета, синуса или многочлена. От выбора нелинейной функции может зависеть сложность конкретной сети, но с любой нелинейностью сеть остаётся универсальным аппроксиматором и при правильном выборе структуры может достаточно точно аппроксимировать функционирование любого непрерывного автомата.Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронСмешанное обучение:Сеть радиально-базисных функцийСжатие данных и Ассоциативная память[править | править вики-текст]Основные статьи: Нейросетевое сжатие данных, Ассоциативная памятьСпособность нейросетей к выявлению взаимосвязей между различными параметрами дает возможность выразить данные большой размерности более компактно, если данные тесно взаимосвязаны друг с другом. Обратный процесс — восстановление исходного набора данных из части информации — называется (авто)ассоциативной памятью. Ассоциативная память позволяет также восстанавливать исходный сигнал/образ из зашумленных/поврежденных входных данных. Решение задачи гетероассоциативной памяти позволяет реализовать память, адресуемую по содержимому[16].Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронОбучение без учителя:Нейронная сеть ХопфилдаАнализ данных[править | править вики-текст]Используемые архитектуры нейросетей[править | править вики-текст]Обучение с учителем:ПерцептронОбучение без учителя:ПерцептронСамоорганизующаяся карта КохоненаНейронная сеть КохоненаОптимизация[править | править вики-текст]Используемые архитектуры нейросетей[править | править вики-текст]Обучение без учителя:Самоорганизующаяся карта КохоненаНейронная сеть КохоненаЭтапы решения задач[править | править вики-текст]Сбор данных для обучения;Подготовка и нормализация данных;Выбор топологии сети;Экспериментальный подбор характеристик сети;Экспериментальный подбор параметров обучения;Собственно обучение Проверка адекватности обучения;Корректировка параметров, окончательное обучение;Вербализация сети[18] с целью дальнейшего использования.Следует рассмотреть подробнее некоторые из этих этапов.Сбор данных для обучения[править | править вики-текст]Выбор данных для обучения сети и их обработка является самым сложным этапом решения задачи. Набор данных для обучения должен удовлетворять нескольким критериям:Репрезентативность — данные должны иллюстрировать истинное положение вещей в предметной области;Непротиворечивость — противоречивые данные в обучающей выборке приведут к плохому качеству обучения сети.Исходные данные преобразуются к виду, в котором их можно подать на входы сети. Каждая запись в файле данных называется обучающей парой или обучающим вектором. Обучающий вектор содержит по одному значению на каждый вход сети и, в зависимости от типа обучения (с учителем или без), по одному значению для каждого выхода сети. Обучение сети на «сыром» наборе, как правило, не даёт качественных результатов. Существует ряд способов улучшить «восприятие» сети.Нормировка выполняется, когда на различные входы подаются данные разной размерности. Например, на первый вход сети подаются величины со значениями от нуля до единицы, а на второй — от ста до тысячи. При отсутствии нормировки значения на втором входе будут всегда оказывать существенно большее влияние на выход сети, чем значения на первом входе. При нормировке размерности всех входных и выходных данных сводятся воедино;Квантование выполняется над непрерывными величинами, для которых выделяется конечный набор дискретных значений. Например, квантование используют для задания частот звуковых сигналов при распознавании речи;Фильтрация выполняется для «зашумленных» данных.Кроме того, большую роль играет само представление как входных, так и выходных данных. Предположим, сеть обучается распознаванию букв на изображениях и имеет один числовой выход — номер буквы в алфавите. В этом случае сеть получит ложное представление о том, что буквы с номерами 1 и 2 более похожи, чем буквы с номерами 1 и 3, что, в общем, неверно. Для того, чтобы избежать такой ситуации, используют топологию сети с большим числом выходов, когда каждый выход имеет свой смысл. Чем больше выходов в сети, тем большее расстояние между классами и тем сложнее их спутать.Выбор топологии сети[править | править вики-текст]Выбирать тип сети следует, исходя из постановки задачи и имеющихся данных для обучения. Для обучения с учителем требуется наличие для каждого элемента выборки «экспертной» оценки. Иногда получение такой оценки для большого массива данных просто невозможно. В этих случаях естественным выбором является сеть, обучающаяся без учителя (например, самоорганизующаяся карта Кохонена или нейронная сеть Хопфилда). При решении других задач (таких, как прогнозирование временных рядов) экспертная оценка уже содержится в исходных данных и может быть выделена при их обработке. В этом случае можно использовать многослойный перцептрон[уточнить] или сеть Ворда.Экспериментальный подбор характеристик сети[править | править вики-текст]После выбора общей структуры нужно экспериментально подобрать параметры сети. Для сетей, подобных перцептрону, это будет число слоев, число блоков в скрытых слоях (для сетей Ворда), наличие или отсутствие обходных соединений, передаточные функции нейронов. При выборе количества слоев и нейронов в них следует исходить из того, что способности сети к обобщению тем выше, чем больше суммарное число связей между нейронами. С другой стороны, число связей ограничено сверху количеством записей в обучающих данных.Экспериментальный подбор параметров обучения[править | править вики-текст]После выбора конкретной топологии необходимо выбрать параметры обучения нейронной сети. Этот этап особенно важен для сетей, обучающихся с учителем. От правильного выбора параметров зависит не только то, насколько быстро ответы сети будут сходиться к правильным ответам. Например, выбор низкой скорости обучения увеличит время схождения, однако иногда позволяет избежать паралича сети. Увеличение момента обучения может привести как к увеличению, так и к уменьшению времени сходимости, в зависимости от формы поверхности ошибки. Исходя из такого противоречивого влияния параметров, можно сделать вывод, что их значения нужно выбирать экспериментально, руководствуясь при этом критерием завершения обучения (например, минимизация ошибки или ограничение по времени обучения).Обучение сети[править | править вики-текст]В процессе обучения сеть в определенном порядке просматривает обучающую выборку. Порядок просмотра может быть последовательным, случайным и т. д. Некоторые сети, обучающиеся без учителя (например, сети Хопфилда), просматривают выборку только один раз. Другие (например, сети Кохонена), а также сети, обучающиеся с учителем, просматривают выборку множество раз, при этом один полный проход по выборке называется эпохой обучения. При обучении с учителем набор исходных данных делят на две части — собственно обучающую выборку и тестовые данные; принцип разделения может быть произвольным. Обучающие данные подаются сети для обучения, а проверочные используются для расчета ошибки сети (проверочные данные никогда для обучения сети не применяются). Таким образом, если на проверочных данных ошибка уменьшается, то сеть действительно выполняет обобщение. Если ошибка на обучающих данных продолжает уменьшаться, а ошибка на тестовых данных увеличивается, значит, сеть перестала выполнять обобщение и просто «запоминает» обучающие данные. Это явление называется переобучением сети или оверфиттингом. В таких случаях обучение обычно прекращают. В процессе обучения могут проявиться другие проблемы, такие как паралич или попадание сети в локальный минимум поверхности ошибок. Невозможно заранее предсказать проявление той или иной проблемы, равно как и дать однозначные рекомендации к их разрешению.Все выше сказанное относится только к итерационным алгоритмам поиска нейросетевых решений. Для них действительно нельзя ничего гарантировать и нельзя полностью автоматизировать обучение нейронных сетей.[источник не указан 1602 дня] Однако, наряду с итерационными алгоритмами обучения, существуют не итерационные алгоритмы, обладающие очень высокой устойчивостью и позволяющие полностью автоматизировать процесс обучения.Проверка адекватности обучения[править | править вики-текст]Даже в случае успешного, на первый взгляд, обучения сеть не всегда обучается именно тому, чего от неё хотел создатель. Известен случай, когда сеть обучалась распознаванию изображений танков по фотографиям, однако позднее выяснилось, что все танки были сфотографированы на одном и том же фоне. В результате сеть «научилась» распознавать этот тип ландшафта, вместо того, чтобы «научиться» распознавать танки[19]. Таким образом, сеть «понимает» не то, что от неё требовалось, а то, что проще всего обобщить.Тестирование качества обучения нейросети необходимо проводить на примерах, которые не участвовали в её обучении. При этом число тестовых примеров должно быть тем больше, чем выше качество обучения. Если ошибки нейронной сети имеют вероятность близкую к одной миллиардной, то и для подтверждения этой вероятности нужен миллиард тестовых примеров. Получается, что тестирование хорошо обученных нейронных сетей становится очень трудной задачей.Классификация по типу входной информации[править | править вики-текст]Аналоговые нейронные сети (используют информацию в форме действительных чисел);Двоичные нейронные сети (оперируют с информацией, представленной в двоичном виде);Образные нейронные сети (оперируют с информацией, представленной в виде образов: знаков, иероглифов, символов).Классификация по характеру обучения[править | править вики-текст]Обучение с учителем — выходное пространство решений нейронной сети известно;Обучение без учителя — нейронная сеть формирует выходное пространство решений только на основе входных воздействий. Такие сети называют самоорганизующимися;Обучение с подкреплением — система назначения штрафов и поощрений от среды.Классификация по характеру настройки синапсов[править | править вики-текст]Сети с фиксированными связями (весовые коэффициенты нейронной сети выбираются сразу, исходя из условий задачи, при этом: {\displaystyle {\boldsymbol {d}}W/dt=0} {\boldsymbol {d}}W/dt=0, где W — весовые коэффициенты сети);Сети с динамическими связями (для них в процессе обучения происходит настройка синаптических связей, то есть {\displaystyle {\boldsymbol {d}}W/dt\not =0} {\boldsymbol {d}}W/dt\not =0, где W — весовые коэффициенты сети).Классификация по времени передачи сигнала[править | править вики-текст]В ряде нейронных сетей активирующая функция может зависеть не только от весовых коэффициентов связей {\displaystyle w_{ij}} w_{ij}, но и от времени передачи импульса (сигнала) по каналам связи {\displaystyle \tau _{ij}} \tau _{ij}. Поэтому в общем виде активирующая (передающая) функция связи {\displaystyle c_{ij}} c_{ij} от элемента {\displaystyle u_{i}} u_{i} к элементу {\displaystyle u_{j}} u_{j} имеет вид: {\displaystyle c_{ij}^{*}=f[w_{ij}(t),u_{i}^{*}(t-\tau _{ij})]} c_{ij}^{*}=f[w_{ij}(t),u_{i}^{*}(t-\tau _{ij})]. Тогда синхронной сетью называют такую сеть, у которой время передачи {\displaystyle \tau _{ij}} \tau _{ij} каждой связи равно либо нулю, либо фиксированной постоянной {\displaystyle \tau } \tau . Асинхронной называют такую сеть у которой время передачи {\displaystyle \tau _{ij}} \tau _{ij} для каждой связи между элементами {\displaystyle u_{i}} u_{i} и {\displaystyle u_{j}} u_{j} своё, но тоже постоянное.Классификация по характеру связей[править | править вики-текст]Сети прямого распространения (Feedforward)[править | править вики-текст]Все связи направлены строго от входных нейронов к выходным. Примерами таких сетей являются перцептрон Розенблатта, многослойный перцептрон, сети Ворда.Рекуррентные нейронные сети[править | править вики-текст]Основная статья: Рекуррентная нейронная сетьСигнал с выходных нейронов или нейронов скрытого слоя частично передается обратно на входы нейронов входного слоя (обратная связь). Рекуррентная сеть Хопфилда «фильтрует» входные данные, возвращаясь к устойчивому состоянию и, таким образом, позволяет решать задачи компрессии данных и построения ассоциативной памяти[20]. Частным случаем рекуррентных сетей являются двунаправленные сети. В таких сетях между слоями существуют связи как в направлении от входного слоя к выходному, так и в обратном. Классическим примером является Нейронная сеть Коско.Радиально-базисные функции[править | править вики-текст]Основная статья: Сеть радиально-базисных функцийРазработаны искусственные нейронные сети, использующие в качестве активационных функций радиально-базисные (также называются RBF-сетями). Общий вид радиально-базисной функции:{\displaystyle f(x)=\phi \left({\frac {x^{2}}{\sigma ^{2}}}\right)} f(x)=\phi \left({\frac {x^{2}}{\sigma ^{2}}}\right), например, {\displaystyle f(x)=e^{-{{x^{2}} \over {\sigma ^{2}}}},} f(x)=e^{-{{x^{2}} \over {\sigma ^{2}}}},где {\displaystyle x} x — вектор входных сигналов нейрона, {\displaystyle \sigma } \sigma  — ширина окна функции, {\displaystyle \phi (y)} \phi (y) — убывающая функция (чаще всего, равная нулю вне некоторого отрезка).Радиально-базисная сеть характеризуется тремя особенностями:Единственный скрытый слой;Только нейроны скрытого слоя имеют нелинейную активационную функцию;Синаптические веса связей входного и скрытого слоев равны единице.Самоорганизующиеся карты[править | править вики-текст]Основная статья: Самоорганизующаяся карта КохоненаТакие сети представляют собой соревновательную нейронную сеть с обучением без учителя, выполняющую задачу визуализации и кластеризации. Является методом проецирования многомерного пространства в пространство с более низкой размерностью (чаще всего, двумерное), применяется также для решения задач моделирования, прогнозирования и др. Является одной из версий нейронных сетей Кохонена[21]. Самоорганизующиеся карты Кохонена служат, в первую очередь, для визуализации и первоначального («разведывательного») анализа данных[22].Сигнал в сеть Кохонена поступает сразу на все нейроны, веса соответствующих синапсов интерпретируются как координаты положения узла, и выходной сигнал формируется по принципу «победитель забирает всё» — то есть ненулевой выходной сигнал имеет нейрон, ближайший (в смысле весов синапсов) к подаваемому на вход объекту. В процессе обучения веса синапсов настраиваются таким образом, чтобы узлы решетки «располагались» в местах локальных сгущений данных, то есть описывали кластерную структуру облака данных, с другой стороны, связи между нейронами соответствуют отношениям соседства между соответствующими кластерами в пространстве признаков.Удобно рассматривать такие карты как двумерные сетки узлов, размещенных в многомерном пространстве. Изначально самоорганизующаяся карта представляет собой сетку из узлов, соединенную между собой связями. Кохонен рассматривал два варианта соединения узлов — в прямоугольную и гексагональную сетку — отличие состоит в том, что в прямоугольной сетке каждый узел соединён с 4-мя соседними, а в гексагональной — с шестью ближайшими узлами. Для двух таких сеток процесс построения сети Кохонена отличается лишь в том месте, где перебираются ближайшие к данному узлу соседи.Начальное вложение сетки в пространство данных выбирается произвольным образом. В авторском пакете SOM_PAK предлагаются варианты случайного начального расположения узлов в пространстве и вариант расположения узлов в плоскости. После этого узлы начинают перемещаться в пространстве согласно следующему алгоритму:Случайным образом выбирается точка данных {\displaystyle x} x.Определяется ближайший к {\displaystyle x} x узел карты (BMU — Best Matching Unit).Этот узел перемещается на заданный шаг по направлению к {\displaystyle x} x. Однако он перемещается не один, а увлекает за собой определённое количество ближайших узлов из некоторой окрестности на карте. Из всех двигающихся узлов наиболее сильно смещается центральный — ближайший к точке данных — узел, а остальные испытывают тем меньшие смещения, чем дальше они от BMU. В настройке карты различают два этапа — этап грубой (ordering) и этап тонкой (fine-tuning) настройки. На первом этапе выбираются большие значения окрестностей и движение узлов носит коллективный характер — в результате карта «расправляется» и грубым образом отражает структуру данных; на этапе тонкой настройки радиус окрестности равен 1-2 и настраиваются уже индивидуальные положения узлов. Кроме этого, величина смещения равномерно затухает со временем, то есть она велика в начале каждого из этапов обучения и близка к нулю в конце.Алгоритм повторяется определенное число эпох (понятно, что число шагов может сильноизменяться в зависимости от задачи).Известные типы сетей[править | править вики-текст]Перцептрон Розенблатта;Сплайн-модель Хакимова;Многослойный перцептрон Розенблатта;Многослойный перцептрон Румельхарта;Сеть Джордана;Сеть Элмана;Сеть Хэмминга;Сеть Ворда;Сеть Хопфилда;ть Кохонена;Нейронный газ[23];Когнитрон;Неокогнитрон;Хаотическая нейронная сеть;Осцилляторная нейронная сеть;Сеть встречного распространения;Сеть радиально-базисных функций (RBF-сеть);Сеть обобщенной регрессии;Сеть Д.Смирнова;Вероятностная сеть;Вероятностная нейронная сеть Решетова;Сиамская нейронная сеть;Сети адаптивного резонанса;Свёрточная нейронная сеть (англ. convolutional neural network);Нечеткий многослойный перцептрон;Импульсная нейронная сеть.Отличия от машин с архитектурой фон Неймана[править | править вики-текст]Вычислительные системы, основанные на искусственных нейронных сетях, обладают рядом качеств, которые отсутствуют в машинах с архитектурой фон Неймана (но присущи мозгу человека):Массовый параллелизм;Распределённое представление информации и вычисления;Способность к обучению и обобщению;Адаптивность;Свойство контекстуальной обработки информации;Толерантность к ошибкам;Низкое энергопотребление.Примеры приложений[править | править вики-текст]Предсказание финансовых временных рядов[править | править вики-текст]Входные данные — курс акций за год. Задача — определить завтрашний курс. Проводится следующее преобразование — выстраивается в ряд курс за сегодня, вчера, за позавчера. Следующий ряд — смещается по дате на один день и так далее. На полученном наборе обучается сеть с 3 входами и одним выходом — то есть выход: курс на дату, входы: курс на дату минус 1 день, минус 2 дня, минус 3 дня. Обученной сети подаем на вход курс за сегодня, вчера, позавчера и получаем ответ на завтра. Нетрудно заметить, что в этом случае сеть просто выведет зависимость одного параметра от трёх предыдущих. Если желательно учитывать ещё какой-то параметр (например, общий индекс по отрасли), то его надо добавить как вход (и включить в примеры), переобучить сеть и получить новые результаты. Для наиболее точного обучения стоит использовать метод ОРО, как наиболее предсказуемый и несложный в реализации.Психодиагностика[править | править вики-текст]Серия работ М. Г. Доррера с соавторами посвящена исследованию вопроса о возможности развития психологической интуиции у нейросетевых экспертных систем[24][25]. Полученные результаты дают подход к раскрытию механизма интуиции нейронных сетей, проявляющейся при решении ими психодиагностических задач. Создан нестандартный для компьютерных методик интуитивный подход к психодиагностике, заключающийся в исключении построения описанной реальности. Он позволяет сократить и упростить работу над психодиагностическими методиками.Хемоинформатика[править | править вики-текст]Нейронные сети широко используются в химических и биохимических исследованиях[26]. В настоящее время нейронные сети являются одним из самых распространенных методов хемоинформатики для поиска количественных соотношений структура-свойство[27][28], благодаря чему они активно используются как для прогнозирования физико-химических свойств и биологической активности химических соединений, так и для направленного дизайна химических соединений и материалов с заранее заданными свойствами, в том числе при разработке новых лекарственных препаратов.Нейроуправление[править | править вики-текст]Основная статья: НейроуправлениеНейронные сети успешно применяются для синтеза систем управления динамическими объектами[29][30]. Нейросети обладают рядом уникальных свойств, которые делают их мощным инструментом для создания систем управления: способностью к обучению на примерах и обобщению данных, способностью адаптироваться к изменению свойств объекта управления и внешней среды, пригодностью для синтеза нелинейных регуляторов, высокой устойчивостью к повреждениям своих элементов в силу изначально заложенного в нейросетевую архитектуру параллелизма.Экономика[править | править вики-текст]Алгоритмы искусственных нейронных сетей нашли широкое применение в экономике[31]. С помощью нейронных сетей решается задача разработки алгоритмов нахождения аналитического описания закономерностей функционирования экономических объектов (предприятие, отрасль, регион). Эти алгоритмы применяются к прогнозированию некоторых «выходных» показателей объектов. Применение нейросетевых методов позволяет решить некоторые проблемы экономико-статистического моделирования, повысить адекватность математических моделей, приблизить их к экономической реальности[32]. Поскольку экономические, финансовые и социальные системы очень сложны и являются результатом человеческих действий и противодействий, создание полной математической модели с учётом всех возможных действий и противодействий является очень сложной (если разрешимой) задачей. В системах подобной сложности естественным и наиболее эффективным является использование моделей, которые напрямую имитируют поведение общества и экономики. Именно это способна предложить методология нейронных сетей[33]."



####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Все константы


#Все символы для вырезки из текста
arr_sumb = [
            ### Частые . " ' ( ) ? ! ; : ###
            "." , "," , "\"" , "\'" , "(" , ")" , "?" , "!" , ";" , ":" , "—",
            # "" , "" , "",
            
            ### Редкие \ / [ ] * # % & № @ ^ < >
            ### « » | _ { } =
            "\\" , "/", "[" , "]" , "*" , "#" , "%" , "&", "№" , "@" , "^" , "<" , ">",
            "«", "»", "|", "_", "{", "}", "=",
            
            ### Невидимые символы ###
            "\n" , "\t" , "\a" , 
            
            # Цифры
            "1","2","3","4","5","6","7","8","9","0",
            
            # Особое
            "&nbsp" , ""
             ]  
             ### Что можно добавить:
             ### арифметические: ' - + = '
             ### Можно добавить все цифры либо найти функц для их уборки
             ### Буквы с ударением бо?льшая
             ### Англ буквы т к они все равно отрежутся
        
# На что меняем
sumb_replace = " ";


################################################

WORD_CUT_PERCENT_UP = 0.5    # вики =0.4
WORD_CUT_PERCENT_DOWN = 0.1  # вики = 0.15
								# числа какие попало
PHRASE_CUT_PERCENT_UP = 0.2   # вики = 0.5
PHRASE_CUT_PERCENT_DOWN = 0.1 # вики = 0.1


# Чтоб можно было сразу сравнивать
WORD_CUT_PERCENT_UP_ACCURATE   = WORD_CUT_PERCENT_UP/100
WORD_CUT_PERCENT_DOWN_ACCURATE = WORD_CUT_PERCENT_DOWN/100

PHRASE_CUT_PERCENT_UP_ACCURATE   = PHRASE_CUT_PERCENT_UP/100
PHRASE_CUT_PERCENT_DOWN_ACCURATE = PHRASE_CUT_PERCENT_DOWN/100


################################################


# Печатать ли заходы/выходы из функций ?
GLOB_Print_func_names = False


##################### Все константы
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Все всякое




# Можно писать так время в разные перем => делать сразу несколько замеров
def My_timer( command , Time_beg = 0.0 , msg="def_msg: " ):
  if command == "Start": return timeit.default_timer()
  
  if command == "Stop" : 
    Time = timeit.default_timer()- Time_beg
    
    #print(Time)
    
    min  = 0
    sec  = 0
    msec = 0
    
    if Time >= 60:
        min = int(Time)%60
        Time = Time - min*60
        
    if Time >= 1:
        sec = int(Time)-1
        Time = Time - sec
        
    msec = int(Time*1000)  # 0.3875471909996122 *1000 = 387.5471909996122  и обрезаем
    
    
    
    if min >= 1:
      print (msg+ str(min) +"м "+str(sec)+"с "+str(msec)+"мс")
      return Time
    
    if sec >= 1:
      print (msg+str(sec)+"с "+str(msec)+"мс")
      return Time
      
    if msec >= 1:
      print (msg+str(msec)+"мс")
      return Time
      
    print (msg+"Меньше мс")
    return Time
    
  # Впихнуть сюда форматирование вывода через printf %
  # Уже не надо
  
# Time = My_timer("Start")
# My_timer("Stop" , Time , "Обсчет 1 чисел занял: " )


# Возвращает СТРОКУ!!!!
# Только для простых строк (не массивов и тд)
# ф-я сильно урезана
def GET_COLORED( num  , targ_color = "красный" ):
	
	colors = {
	  "черный"   : 30 ,
	  "красный"  : 31,
	  "зеленый"  : 32,
	  "желтый"   : 33,
	  "синий"    : 34,
	  "пурпурный" : 35,
	  "голубой"  : 36,
	  "белый"    : 37
	}
	
	
	####################################################

	return "\x1b["+ str(colors.get(targ_color) ) + "m" + str(num) + "\x1b[0m"
	  


def Print_current_func_name( beg_or_end ):
  
  if GLOB_Print_func_names != True:
  	return
  
  
  if beg_or_end == "beg":
    print ("######### Вход в функцию " + inspect.stack()[1][3] )
    return
    
  if beg_or_end == "end":
    print ("######### Выход из функции " + inspect.stack()[1][3] )
    return


##################### Все всякое
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Все print








def PRINT_norm_percents_from_rel_DICT( Dict , mode , kol = 3 ):
	
	print ("#"*30)
	

	
	


	mnojitel = 10000
	
	for i in range( 1 , kol ):
		mnojitel *= 10
	
	
	
	if mode == "WORD":
		print ("Условия обрезки для WORDS: частота между " + str(WORD_CUT_PERCENT_DOWN) + "% и " + str(WORD_CUT_PERCENT_UP) + "%" )
		UP_perc   = WORD_CUT_PERCENT_UP_ACCURATE
		DOWN_perc = WORD_CUT_PERCENT_DOWN_ACCURATE
	
	if mode == "PHRASE":
		print ("Условия обрезки для PHRASES: частота между " + str(PHRASE_CUT_PERCENT_DOWN) + "% и " + str(PHRASE_CUT_PERCENT_UP) + "%" )
		UP_perc   = PHRASE_CUT_PERCENT_UP_ACCURATE
		DOWN_perc = PHRASE_CUT_PERCENT_DOWN_ACCURATE
	
	
	
	
	for key in Dict.keys():
	
		int_percent = int( Dict[key]*100 )
		
		numbs_after_dot = int( Dict[key]*mnojitel )
			
		# 0.001
		str_num_perc = str(int_percent) + "." + str(numbs_after_dot) + "-%"
		
		
		# Вышел за диапазон
		if Dict[key] <= DOWN_perc  or  Dict[key] >= UP_perc:
			str_num_perc_colored = GET_COLORED( str_num_perc , "красный" )
		else: # норм
			str_num_perc_colored = GET_COLORED( str_num_perc , "зеленый" )
		
		
		print ( str_num_perc_colored + ' :' , "\'"+key+"\'")


	return




# Вывод списка в формате =    "2 : новых" или наоборот
# dict = словарь   mode = key/val/all
#all =дефолт с \n= ('ключ',знач)    val =   знач : ключ      key = наоборот
def print_dict( dict , what_first ):
  Print_current_func_name("beg")
  
  # Если пустой
  #if len(dict) == 0:
  if dict is None:  
    print("Словарь под вывод пуст!!!")
    return
  
  if what_first == "val":
     # 2 : новых
     for key in dict.keys():
        print ( dict[key] , ':' , "\'"+key+"\'")
  
        
  if what_first == "key":
     for key in dict.keys(): # вывод элементов словаря (ключ - значение) по алфавиту
        print(key , ':', dict[key])#, end='\n')
  
     #for w in sorted(dict, key=dict.get, reverse=True):  print ( w, dict[w] )
     #for key in dict.keys(): print ( dict[key] , ':' , key)
       
        
  if what_first == "all": 
     # новых : 2 
     for one_pair in dict.items():
       print (one_pair) 

  
  Print_current_func_name("end")
  return







##################### Все print
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Получение данных


# input НЕ ВЫВОЗИТ тескс с переводои строки(enter)  читает только до него
# ВСЕ связаное с введением начального текста/ов
# Возврат: одна текстовая строка(ТОЛЬКО слова и пробелы)
def GET_text_string():
    
    Print_current_func_name("beg")
    #####################
    ###### Получение текста (ЛЮБЫМ способом)
    
    #Если перем с нач текстом нет, то вводим с клавы
    if 'begin_text' not in globals():
      text_string = input("Введите начальный текст: ")
    else:
      text_string = begin_text #это затычка
    
    #print("\n### Исходный текст: \"" + text_string + "\"")
    
    ###### Получение текста (ЛЮБЫМ способом)
    #####################
    ######
    

    
    ######
    #####################
    ###### Первичная обработка
    
    for sumb in arr_sumb:
        text_string = text_string.replace( sumb , sumb_replace )
    
    #print("### После replace: \"" + text_string + "\"")
    
    # Удаялем пробелы в начале и в конце строки
    text_string = text_string.strip( " " ) # или strip( ["","" ... ] )
      
    #Удаляем двойные пробелы  
    while text_string.count('  ')>0:
      text_string = text_string.replace('  ', ' ')
      
    # В нижний регистр
    text_string = text_string.lower() 

    ###### Первичная обработка
    #####################
    ###### Вывод
    
    #print("\n### Финальный текст: \"" + text_string + "\"")
    
    Print_current_func_name("end")
    return text_string
    

##################### Получение данных
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Перегон в LIST


# Вход: Полностью отформатированая строка с текстом (только слова и пробелы) 
# Возврат: Массив (1 слово = 1 элем) с дублями
def GET_aray_words_from_string( fin_text ):
  Print_current_func_name("beg")
  

  #тут еще что-нибудь
  arr_words = fin_text.split(" ")
  
  
  
  #####################
  ###### Вывод
  
  #print("### Массив слов: \n", "\n\"".join(arr_words)+"\"") # В столбик (криво)
  #print("\n### Массив слов: \n", arr_words )
  #print("\n")
  
  Print_current_func_name("end")
  return arr_words





# Подсчет слов и запись в список
def GET_LIST_ALL_PHRASES( arr_words ):
	Print_current_func_name("beg")
  



	list_phrases = []


	i = 0

	# Надо пропустить 1 итерацию
	for word in arr_words:
  	
  	
  	
  		#Небольшой костыль => сделать for word in arr_words.(???) ) чтоб пропускал 1 итерацию
		if i == 0:
			i = 99
			first_word = word
			continue
		
					
		second_word = word
		
		fs_word = first_word + " " + second_word
		
		first_word = word


		list_phrases.append( fs_word )
    
    
    

  
	Print_current_func_name("end")
	return list_phrases




##################### Перегон в LIST
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Подсчет относительной частоты


# Считает по кол-ву таких элементов в списке
# Подсчет слов и запись в список
def Calc_relative_frequency_from_list( arr_words ):
  Print_current_func_name("beg")



  CNT_WORDS_IN_ALL_TEXT = len(arr_words)

  
  
  
  
  # Создаем пустой список для частот слов
  # Вормат   ключ=слово : кол-во
  dict_words_relative_frequency = {  }

  # Каждое слово
  for one_word in arr_words:

    ### ПЕРЕПИСАТЬ т к на болших будет очень тормозить
    
    #for one_key in dict_words_frequency.keys():  # фор и иф точно катят
      #if one_word == one_key:
      
      
      
    # Если слово уже есть в списке - пропускаем т к оно уже подсчитано
    if one_word in dict_words_relative_frequency.keys():  # робит
      #print ("Найден дубль слова: "+ one_word)
      continue
    
    
    # Получаем количество повторов  (можно пихнуть сразу вниз)
    kol = arr_words.count( one_word )

    	
    
    relative_freq = kol / CNT_WORDS_IN_ALL_TEXT
    
    
    # Пишем
    dict_words_relative_frequency.update( { one_word : relative_freq } )
    
    
    
  Print_current_func_name("end")
  return dict_words_relative_frequency



##################### Подсчет относительной частоты
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Сортировки


# Вернет OrderedDict
# Отсортирует по словам, а ПОТОМ по числам (т е у кучи слов со знач '1' будет своя сортировка)
def sort_dict( dict ):
  Print_current_func_name("beg")


  #отсортирует по возрастанию ключей словаря  РОБИТ
  dict_sorted_keys = OrderedDict(sorted(dict.items(), key=lambda t: t[0]))  

  # РОБИТ  внизу большие
  dict_sorted_keys_vals = OrderedDict(sorted(dict_sorted_keys.items(), key=itemgetter(1)))
  
  
  Print_current_func_name("end")
  return dict_sorted_keys_vals




##################### Сортировки
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################
##################### Обрезка краев


# Вернет обычный словарь
# Обрезка краев словаря на N %
def dict_remove_relative_edges( sorted_dict , percent_down , percent_up):
	Print_current_func_name("beg")
  
  
	dict_length = len(sorted_dict)
	print("Длина входного словаря: " + str(dict_length))
  
  
  
	# Создаем новый словарь (!!!!! НЕ OrderedDict - простой словарь)
	dict_relative_edged = {  }

	for key in sorted_dict.keys():
		
		if float( sorted_dict.get(key) ) >= percent_up   or   float( sorted_dict.get(key) ) <= percent_down :
        	#print("\nВыкинут ключ:" , key )
        	#print("% Содержания:" ,sorted_dict.get(key) )
			continue
		
		
		dict_relative_edged.update( { key : sorted_dict[key] } )
    

  
	print( "Длина Выходного словаря: " ,len(dict_relative_edged))
	
	Print_current_func_name("end")
	return dict_relative_edged



##################### Обрезка краев
####################################################################################
####################################################################################
####################################################################################
####################################################################################
####################################################################################





'''
# ВСЕ до обрезки краев в 1 строчку
print_dict ( #dict_remove_relative_edges(
                            sort_dict( 
                              Calc_relative_frequency_from_list(
                                 GET_aray_words_from_string(
                                   GET_text_string(             ) # получаем текст
                                                           ) # делим на слова
                                                  ) # вычисл частоты
                                      ) # сортируем список
              #            , 5) # обрезка
          , "val" ) #print 

# эта строка есть ниже
'''

print("####################################################")
print("####################################################")
print("####################################################")
print("####################################################")
print("####################################################")
print("####################################################")






Time_beg_body = My_timer("Start")

####################################################################################
####################################################################################
####################################################################################
##################### Забор слов


arr_words_orig_order_final = GET_aray_words_from_string( GET_text_string() )


##################### Забор слов
####################################################################################
####################################################################################
####################################################################################
##################### Все для WORDS



dict_words_relative_frequency = Calc_relative_frequency_from_list( arr_words_orig_order_final)
#print_dict(dict_words_relative_frequency , "val")


dict_words_relative_frequency_sort = sort_dict(dict_words_relative_frequency )
#print_dict(dict_words_relative_frequency_sort , "val")


dict_words_relative_frequency_sort_edged = dict_remove_relative_edges(
	dict_words_relative_frequency_sort , WORD_CUT_PERCENT_DOWN_ACCURATE , WORD_CUT_PERCENT_UP_ACCURATE )
#print_dict(dict_words_relative_frequency_sort_edged , "val")



##################### Все для WORDS
####################################################################################
####################################################################################
####################################################################################
##################### Все для PHRASES



LIST_ALL_PHRASES = GET_LIST_ALL_PHRASES(arr_words_orig_order_final)
#for i in LIST_ALL_PHRASES:		print(i)


dict_PHRASES_relative_frequency = Calc_relative_frequency_from_list( LIST_ALL_PHRASES )
#print_dict(dict_PHRASES_relative_frequency , "val")


dict_PHRASE_relative_frequency_sort = sort_dict(dict_PHRASES_relative_frequency )
#print_dict(dict_PHRASE_relative_frequency_sort , "val")


dict_PHRASE_relative_frequency_sort_edged = dict_remove_relative_edges( 
	dict_PHRASE_relative_frequency_sort , PHRASE_CUT_PERCENT_DOWN_ACCURATE , PHRASE_CUT_PERCENT_UP_ACCURATE)
#print_dict(dict_PHRASE_relative_frequency_sort_edged , "val")

##################### Все для PHRASES
####################################################################################
####################################################################################
####################################################################################
##################### Выводы




# Вывод ВСЕХ слов
PRINT_norm_percents_from_rel_DICT( dict_words_relative_frequency_sort , "WORD" )

# Вывод после обрезки
PRINT_norm_percents_from_rel_DICT( dict_words_relative_frequency_sort_edged , "WORD" )



###############################################



# Вывод ВСЕХ словосочитаний
PRINT_norm_percents_from_rel_DICT( dict_PHRASE_relative_frequency_sort , "PHRASE" )

# Вывод после обрезки
PRINT_norm_percents_from_rel_DICT( dict_PHRASE_relative_frequency_sort_edged , "PHRASE" )




##################### Выводы
####################################################################################
####################################################################################
####################################################################################
    
    
    
    
    
    
    
print("\n\n")
print( "################ Итоги ##################" ) 

print( "Всего входных слов:", len(arr_words_orig_order_final))
print( "Всего слов осталось:", len(dict_words_relative_frequency_sort_edged))
print("")
print( "Всего фраз:", len(LIST_ALL_PHRASES))
print( "Всего фраз осталось:", len(dict_PHRASE_relative_frequency_sort_edged))
print("")

My_timer("Stop" , Time_beg_body , "Обсчет и вывод всего этого занял: " )


print( "############### END ALL #################" )
### end














# print(locals())
    

'''
Списки имеют большой набор функций:
append , extend — добавление;
insert — вставка;
index — найти индекс первого вхождения конкретного элемента;
count — подсчет повторов элемента;
remove , del — удаление элемента;
sort — сортировка;
reverse — реверс;
pop — извлечение элемента;






Если вы строите словарь со словами в виде ключей и количеством вхождений каждого слова в качестве значения, упрощается здесь как:

from collections import defaultdict
d = defaultdict(int)
for w in text.split():
  d[w] += 1




Функции/методы словаря
dict() — создание словаря;
len() — возвращает число пар;
clear() — удаляет все значения из словаря;
copy() — создает псевдокопию словаря;
deepcopy() — создает полную копию словаря;
fromkeys() — создание словаря;
get() — получить значение по ключу;
has_key() — проверка значения по ключу;
items() — возвращает список значений;
iteriyems() — возвращает итератор;
keys() — возвращает список ключей;
iterkeys() — возвращает итератор ключей;
pop() — извлекает значение по ключу;
popitem() — извлекает произвольное значение;
update() — изменяет словарь;
values() — возвращает список значений;
itervalues() — возвращает итератор на список значений.
in — оператор, проверяет наличие значения по ключу;
del — оператор, удаляет пару по ключу;
dict() — конструирует словарь с помощью последовательности.

'''









